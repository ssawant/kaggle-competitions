{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google Landmark Recognition 2021.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOuHWu1Fl3ZiRbsmBWBoqo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssawant/kaggle-competitions/blob/main/Google_Landmark_Recognition_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjBE8nujxZXn",
        "outputId": "57a172c2-9b5c-4591-a222-ada2dc53ee53"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAScNLm94fUF"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bQsMN_P4mha"
      },
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZE1B4D49Hyy",
        "outputId": "c30b5cf6-724e-432d-abf0-a4f787363197"
      },
      "source": [
        "!kaggle --version"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle API 1.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW35zUuaK1a-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hler_Lay5Zoq",
        "outputId": "7cdfeae3-889c-4b32-ffbe-07beb468cfc0"
      },
      "source": [
        "# Install EfficientnetV2 model\n",
        "\n",
        "! pip install git+https://github.com/sebastian-sz/efficientnet-v2-keras@main --no-deps\n",
        "\n",
        "from efficientnet_v2 import EfficientNetV2S as ENetV2\n",
        "\n",
        "# efficientnet_v2s\n",
        "# WEIGHTS = \"imagenet++\" #imagenet-21k-ft1k\n",
        "WEIGHTS = \"imagenet-21k-ft1k\"\n",
        "IMG_SIZE = 384\n",
        "IMG_PAD = 16  # for random cropping"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/sebastian-sz/efficientnet-v2-keras@main\n",
            "  Cloning https://github.com/sebastian-sz/efficientnet-v2-keras (to revision main) to /tmp/pip-req-build-p9ywxb5t\n",
            "  Running command git clone -q https://github.com/sebastian-sz/efficientnet-v2-keras /tmp/pip-req-build-p9ywxb5t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9v2pD6XKrX2"
      },
      "source": [
        "# Init Parameters\n",
        "\n",
        "N_CLASSES = 81313\n",
        "N_RECORDS = 1580470\n",
        "\n",
        "SEED = 4672\n",
        "random.seed(SEED)\n",
        "\n",
        "# batching and tuning strategies for TPUs\n",
        "BUFFER = 2048  # for shuffling\n",
        "BATCH = 128 #* strategy.num_replicas_in_sync\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "823vPLTUOK8Q",
        "outputId": "a33c9da6-e122-4933-9942-db1ace8faf0d"
      },
      "source": [
        "# Kaggle download dataset\n",
        "!kaggle datasets list -s 'Landmark Recognition 2021 TFRecords 384'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                            title                                              size  lastUpdated          downloadCount  \n",
            "-------------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "markwijkhuizen/landmark-recognition-2021-tfrecords-384-part-1  Landmark Recognition 2021 TFRecords 384 Part 1     17GB  2021-08-18 18:26:00             26  \n",
            "markwijkhuizen/landmark-recognition-2021-tfrecords-384-part-2  Landmark Recognition 2021 TFRecords 384 Part 2     17GB  2021-08-18 18:26:29             15  \n",
            "markwijkhuizen/landmark-recognition-2021-tfrecords-384-part-3  Landmark Recognition 2021 TFRecords 384 Part 3     17GB  2021-08-18 18:26:56             13  \n",
            "ankursingh12/resized-plant2021                                 resized_plant2021                                   1GB  2021-03-17 08:45:09           1034  \n",
            "jpmiller/connect-four-datasets                                 Simulation Replays for Halite                       4GB  2020-11-13 23:41:17            174  \n",
            "sarques/siim128x128-mix                                        SIIM-ISIC Melanoma 384x384                        669MB  2020-06-23 13:54:08             15  \n",
            "uciml/ct-slice-localization                                    CAT Scan Localization                              18MB  2017-09-06 22:01:03            501  \n",
            "sorour/38cloud-cloud-segmentation-in-satellite-images          38-Cloud: Cloud Segmentation in Satellite Images   12GB  2021-04-12 19:12:54           4055  \n",
            "wrrosa/hubmap-tfrecords-768-384                                HuBMAP tfrecords 768 384                           11GB  2021-01-08 13:42:03              6  \n",
            "sameeharahman/preprocessed-snake-images                        Pre-processed Snake Images                          1GB  2020-03-30 08:30:24            290  \n",
            "hamishdickson/siimisic-melanoma-resized-images-384             siimisic-melanoma-resized-images-384                3GB  2020-07-03 17:09:07              0  \n",
            "elikplim/car-evaluation-data-set                               Car Evaluation Data Set                             5KB  2017-09-01 04:05:49           8382  \n",
            "ragnar123/shopee-effb3-model-384                               Shopee Effb3 Model 384                            754MB  2021-05-01 14:26:50              5  \n",
            "ragnar123/shopee-tf-records-4-384                              Shopee TF Records 4 384                             3GB  2021-04-19 15:10:27              4  \n",
            "hidehisaarai1213/g2net-cqt-tfrecord-train-384-2-3              G2Net CQT TFRecord  train 384 2 3                  17GB  2021-08-11 09:32:15              0  \n",
            "hidehisaarai1213/g2net-cqt-tfrecord-train-384-4-5              G2Net CQT TFRecord  train 384 4 5                  17GB  2021-08-11 09:32:51              0  \n",
            "hidehisaarai1213/g2net-cqt-tfrecord-train-384-8-9              G2Net CQT TFRecord  train 384 8 9                  17GB  2021-08-11 09:33:20              0  \n",
            "hidehisaarai1213/g2net-cqt-tfrecord-train-384-12-13            G2Net CQT TFRecord  train 384 12 13                17GB  2021-08-11 09:33:58              0  \n",
            "hidehisaarai1213/g2net-cqt-tfrecord-train-384-14-15            G2Net CQT TFRecord train 384 14 15                 17GB  2021-08-11 10:50:09              0  \n",
            "ragnar123/landmark-tfrecords-384                               landmark_tfrecords_384                               0B  2020-09-02 14:47:30            199  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP-SKYLlOpKk",
        "outputId": "f84f9972-b81b-4097-9d2f-3e31347e4b4e"
      },
      "source": [
        "!kaggle datasets download 'markwijkhuizen/landmark-recognition-2021-tfrecords-384-part-1'\n",
        "!kaggle datasets download 'markwijkhuizen/landmark-recognition-2021-tfrecords-384-part-2'\n",
        "!kaggle datasets download 'markwijkhuizen/landmark-recognition-2021-tfrecords-384-part-3'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading landmark-recognition-2021-tfrecords-384-part-1.zip to /content\n",
            "100% 17.2G/17.2G [02:36<00:00, 118MB/s]\n",
            "100% 17.2G/17.2G [02:36<00:00, 118MB/s]\n",
            "Downloading landmark-recognition-2021-tfrecords-384-part-2.zip to /content\n",
            "100% 17.2G/17.2G [02:31<00:00, 169MB/s]\n",
            "100% 17.2G/17.2G [02:32<00:00, 122MB/s]\n",
            "Downloading landmark-recognition-2021-tfrecords-384-part-3.zip to /content\n",
            "100% 17.2G/17.2G [02:32<00:00, 118MB/s]\n",
            "100% 17.2G/17.2G [02:32<00:00, 121MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58w10HPIRFdP"
      },
      "source": [
        "!unzip '/content/landmark-recognition-2021-tfrecords-384-part-1.zip' -d landmark-recognition-2021-tfrecords-384-part-1\n",
        "!unzip '/content/landmark-recognition-2021-tfrecords-384-part-2.zip' -d landmark-recognition-2021-tfrecords-384-part-2\n",
        "!unzip '/content/landmark-recognition-2021-tfrecords-384-part-3.zip' -d landmark-recognition-2021-tfrecords-384-part-3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feb-r9sjXqZo"
      },
      "source": [
        "\"\"\"\n",
        "gs://kds-3c3b1b2c873502801f5b1fd7cfde0ff9b1d2d27c43b2302c2886a4f5\n",
        "gs://kds-4bc4f0d3ed65b1df5fc5b5348504a475b64efda176a19e3f6afab4dd\n",
        "gs://kds-5d1316b17087454bfff62f35c28ab619b8f98fa37c7afdb8f64a0311\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tppA4vcoX-oc"
      },
      "source": [
        "TFR_1, TFR_2, TFR_3 = '/content/landmark-recognition-2021-tfrecords-384-part-1', '/content/landmark-recognition-2021-tfrecords-384-part-2', '/content/landmark-recognition-2021-tfrecords-384-part-3'\n",
        "TFRECORDS = (\n",
        "    tf.io.gfile.glob(f'{TFR_1}/*.tfrecords') +\n",
        "    tf.io.gfile.glob(f'{TFR_2}/*.tfrecords') +\n",
        "    tf.io.gfile.glob(f'{TFR_3}/*.tfrecords')\n",
        ")\n",
        "\n",
        "# First shuffle\n",
        "random.shuffle(TFRECORDS)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-pmEZZYYygw"
      },
      "source": [
        "GCS_TFR_1 = 'gs://kds-3c3b1b2c873502801f5b1fd7cfde0ff9b1d2d27c43b2302c2886a4f5'\n",
        "GCS_TFR_2 = 'gs://kds-4bc4f0d3ed65b1df5fc5b5348504a475b64efda176a19e3f6afab4dd'\n",
        "GCS_TFR_3 = 'gs://kds-5d1316b17087454bfff62f35c28ab619b8f98fa37c7afdb8f64a0311'\n",
        "\n",
        "GCS_TFRECORDS = (\n",
        "    tf.io.gfile.glob(f'{GCS_TFR_1}/*.tfrecords') +\n",
        "    tf.io.gfile.glob(f'{GCS_TFR_2}/*.tfrecords') +\n",
        "    tf.io.gfile.glob(f'{GCS_TFR_3}/*.tfrecords')\n",
        ")\n",
        "\n",
        "# First shuffle\n",
        "random.shuffle(GCS_TFRECORDS)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E6tjhRC3oCS",
        "outputId": "42c166be-fd0e-4b07-8044-c85fce1e6fe7"
      },
      "source": [
        "# TPU Boilerplate\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
        "    strategy = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.16.231.130:8470']\n",
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.16.231.130:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.16.231.130:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5--t_YpZSp2"
      },
      "source": [
        "# Define the mapping function to extract, transform and load images for training\n",
        "\n",
        "# TF Record format defined during creation\n",
        "# www.kaggle.com/markwijkhuizen/google-landmark-recognition-2021-tfrecords-res-384\n",
        "\n",
        "tfrecord_format = {\n",
        "    'image': tf.io.FixedLenFeature([], tf.string),\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "}\n",
        "\n",
        "def decode_tfrecord(record_bytes):\n",
        "  features = tf.io.parse_single_example(record_bytes, tfrecord_format)\n",
        "\n",
        "  # load the image and cast to bfloat16\n",
        "  img = tf.io.decode_jpeg(features['image'])\n",
        "  img = tf.cast(img, tf.bfloat16)\n",
        "\n",
        "  # reshape image to require resoluation\n",
        "  img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "  # agument\n",
        "  img = tf.image.random_flip_left_right(img)\n",
        "  # img = tf.image.random_crop(img, (IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "  # normalize\n",
        "  img = tf.math.divide(img, 127.5)\n",
        "  img = tf.math.subtract(img, 1.0)\n",
        "\n",
        "  # one hot encode label\n",
        "  label = tf.cast(features['label'], tf.int32)\n",
        "  label_one_hot = tf.one_hot(\n",
        "      label,\n",
        "      N_CLASSES,\n",
        "      dtype= tf.uint8\n",
        "  )\n",
        "\n",
        "  return {\"image\": img, \"label\": label}, label_one_hot\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "    # improve performance by ignoring order\n",
        "    data_options = tf.data.Options()\n",
        "    data_options.experimental_deterministic = False\n",
        "\n",
        "    # extract dataset from TFRecords\n",
        "    ds = tf.data.TFRecordDataset(GCS_TFRECORDS, num_parallel_reads=AUTO)\n",
        "    ds = ds.with_options(data_options).shuffle(BUFFER)\n",
        "    ds = ds.map(decode_tfrecord, num_parallel_calls=AUTO)\n",
        "    ds = ds.repeat().batch(BATCH).prefetch(AUTO)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tieF6aq9tqyw"
      },
      "source": [
        "# ArcFace and GeM custom layers\n",
        "\n",
        "class ArcMarginPenalty(tf.keras.layers.Layer):\n",
        "    \"\"\" ArcFace: Additive Angular Margin Loss\n",
        "    Loss function to enhance discriminative power of DNNs.\n",
        "    \n",
        "    Applies an additive angular margin penalty that\n",
        "    increases the geodesic distance gap (i.e. separability)\n",
        "    between closest classes when applying softmax.\n",
        "    \n",
        "    https://arxiv.org/abs/1801.07698\n",
        "    https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution\n",
        "    https://github.com/peteryuX/arcface-tf2\n",
        "    \n",
        "    n_classes -- number of unique classes\n",
        "    margin -- margin magnitude\n",
        "    scale -- constant scaling factor for output logists\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes, margin=0.5, scale=64, **kwargs):\n",
        "        self.n_classes = n_classes\n",
        "        self.update_margin_scale(margin, scale)\n",
        "        super(ArcMarginPenalty, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ArcMarginPenalty, self).get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            'margin': self.margin,\n",
        "            'scale': self.scale,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginPenalty, self).build(input_shape[0])\n",
        "        self.w = self.add_weight(\n",
        "            \"weights\",\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes)\n",
        "        )\n",
        "\n",
        "    def update_margin_scale(self, margin, scale):\n",
        "        self.margin = margin\n",
        "        self.scale = scale\n",
        "        self.cos_m = tf.identity(tf.math.cos(self.margin))\n",
        "        self.sin_m = tf.identity(tf.math.sin(self.margin))\n",
        "        self.th = tf.identity(tf.math.cos(np.pi - self.margin))\n",
        "        self.mm = tf.multiply(self.sin_m, self.margin)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "\n",
        "        embds, labels = inputs\n",
        "\n",
        "        if training is None:\n",
        "            training = tf.keras.backend.learning_phase()\n",
        "\n",
        "        # calculate cos(theta)\n",
        "        cos_t = tf.matmul(\n",
        "            tf.nn.l2_normalize(embds, axis=1),\n",
        "            tf.nn.l2_normalize(self.w, axis=0)\n",
        "        )\n",
        "\n",
        "        # for inference return cosine similarity\n",
        "        if not training:\n",
        "            return cos_t\n",
        "\n",
        "        # add margin, i.e. cos(theta+m)\n",
        "        sin_t = tf.math.sqrt(1.0 - tf.math.pow(cos_t, 2))\n",
        "        cos_mt = cos_t * self.cos_m - sin_t * self.sin_m\n",
        "\n",
        "        # ensure theta+m lies in the range [0, pi]\n",
        "        cos_mt = tf.where(cos_t > self.th, cos_mt, cos_t - self.mm)\n",
        "\n",
        "        # retrieve logists and scale\n",
        "        mask = tf.one_hot(labels, depth=self.n_classes, dtype=cos_t.dtype)\n",
        "        output = (mask * cos_mt) + ((1.0 - mask) * cos_t)\n",
        "        return output * self.scale\n",
        "\n",
        "\n",
        "class GeneralizedMeanPooling(tf.keras.layers.Layer):\n",
        "    \"\"\" Compute the generalized mean of each channel in a tensor\n",
        "    trainable parameter p increases contrast of the\n",
        "    pooled feature map to focus on salient features of the\n",
        "    image. (1: average pooling, infinite: max pooling)\n",
        "\n",
        "    pool_size -- downscale factor\n",
        "    init_norm -- initial magnitude for p\n",
        "    normalize -- apply L2-normalisation to output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pool_size, init_norm=3.0, normalize=False, **kwargs):\n",
        "        self.pool_size = pool_size\n",
        "        self.init_norm = init_norm\n",
        "        self.normalize = normalize\n",
        "        super(GeneralizedMeanPooling, self).__init__(**kwargs)\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = super(GeneralizedMeanPooling, self).get_config().copy()\n",
        "        config.update({\n",
        "            'pool_size': self.pool_size,\n",
        "            'init_norm': self.init_norm,\n",
        "            'normalize': self.normalize,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(GeneralizedMeanPooling, self).build(input_shape)\n",
        "        feature_size = input_shape[-1]\n",
        "        self.p = self.add_weight(\n",
        "            name='norms',\n",
        "            shape=(feature_size,),\n",
        "            initializer=tf.keras.initializers.constant(self.init_norm),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        x = tf.math.maximum(x, 1e-6)\n",
        "        x = tf.pow(x, self.p)\n",
        "        x = tf.nn.avg_pool(x, self.pool_size, self.pool_size, \"VALID\")\n",
        "        x = tf.pow(x, 1.0 / self.p)\n",
        "        if self.normalize:\n",
        "            x = tf.nn.l2_normalize(x, 1)\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, input_shape[-1]])"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Z18E-kFLp5"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"\n",
        "\n",
        "efficientnet_v2s = 'https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/classification/2'\n",
        "\n",
        "feature_extractor_model = hub.load(efficientnet_v2s)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QpZn2TYfu8Z"
      },
      "source": [
        "from tensorflow.keras.applications.efficientnet import EfficientNetB4\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvkOjdO53O6h"
      },
      "source": [
        "# Define the model\n",
        "\n",
        "# first step: build model with frozen pretrained weights\n",
        "# - at this stage a large learning rate is used\n",
        "# - a small arcmargin is used in the initial stages\n",
        "def build_model(n_classes):\n",
        "\n",
        "    # two inputs: images and labels\n",
        "    image = keras.layers.Input(\n",
        "        shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "        name=\"image\",\n",
        "        dtype=tf.bfloat16\n",
        "    )\n",
        "    \n",
        "    label = keras.layers.Input(\n",
        "        shape=(),\n",
        "        name=\"label\",\n",
        "        dtype=tf.int32\n",
        "    )\n",
        "\n",
        "    # # load model without top + freeze pretrained weights\n",
        "    # model = ENetV2(\n",
        "    #     input_tensor=image,\n",
        "    #     include_top=False,\n",
        "    #     pooling=None,\n",
        "    #     weights=WEIGHTS\n",
        "    # )\n",
        "    # model.trainable = False\n",
        "\n",
        "    model = EfficientNetB4(\n",
        "        include_top=False, \n",
        "        weights='imagenet', \n",
        "        input_tensor=image\n",
        "        )\n",
        "  \n",
        "    model.trainable = False\n",
        "    \n",
        "    # pool model output\n",
        "    x = model.output\n",
        "  \n",
        "    x = GeneralizedMeanPooling(12)(x) # imgsize / 32\n",
        "\n",
        "    # squeeze\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(\n",
        "        512,\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        name=\"squeeze_dense\"\n",
        "    )(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.PReLU()(x)  # trainable leaky ReLU\n",
        "\n",
        "    # arcface\n",
        "    output = ArcMarginPenalty(\n",
        "        n_classes,\n",
        "        margin=0.3,\n",
        "        scale=40,\n",
        "        name=\"arcface\",\n",
        "        dtype=tf.float32\n",
        "    )([x, label])\n",
        "\n",
        "    # Compile\n",
        "    model = keras.Model(inputs=[image, label], outputs=[output])\n",
        "    # model = keras.Model(inputs=[image], outputs=[output])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# after initial training all layers except for batchnorm are unfrozen\n",
        "# - the learning rate is dramatically decreased\n",
        "# - margin is increased to increase discriminative power\n",
        "def unfreeze_model(model):\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if not isinstance(layer, keras.layers.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "\n",
        "    model.get_layer(\"arcface\").update_margin_scale(0.5, 40)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgnJjJfx3SD0"
      },
      "source": [
        "# build model within scope to use TPUs\n",
        "with strategy.scope():\n",
        "    model = build_model(N_CLASSES)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8w9JBwp3kp1",
        "outputId": "a2df2609-af46-45b6-ded0-0e8e3487eee9"
      },
      "source": [
        "# Initial training stage - no validation is used\n",
        "# Number of epochs was determined from previous investigations\n",
        "\n",
        "EPOCHS = 5\n",
        "STAGES = int(np.ceil(N_RECORDS / BATCH))\n",
        "\n",
        "tape = model.fit(\n",
        "    ds,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=STAGES,\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "ax[0].plot(tape.history[\"loss\"], label=\"train\")\n",
        "ax[1].plot(tape.history[\"accuracy\"], label=\"train\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1544/1544 [==============================] - 1379s 866ms/step - loss: nan - accuracy: 1.2650e-06\n",
            "Epoch 2/5\n",
            "1544/1544 [==============================] - 1308s 847ms/step - loss: nan - accuracy: 2.5300e-06\n",
            "Epoch 3/5\n",
            "1308/1544 [========================>.....] - ETA: 3:32 - loss: nan - accuracy: 2.9864e-06"
          ]
        }
      ]
    }
  ]
}